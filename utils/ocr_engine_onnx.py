"""
–î–≤–∏–∂–æ–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ ONNX Runtime

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–æ—Ç–æ–≤—ã–µ ONNX –º–æ–¥–µ–ª–∏ –æ—Ç PaddleOCR:
- Detection model (–¥–µ—Ç–µ–∫—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤)
- Recognition model (—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞)
- Classification model (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏)

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ ONNX Runtime:
- –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ GPU (NVIDIA CUDA, AMD/Intel DirectML)
- –ù–∞ 30% –±—ã—Å—Ç—Ä–µ–µ CPU –≤–µ—Ä—Å–∏–∏ PaddleOCR
- –ú–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–µ–π (~20 –ú–ë vs 200-500 –ú–ë)
- –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è 8-10 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —ç–º—É–ª—è—Ç–æ—Ä–æ–≤
"""

import os
import re
import cv2
import numpy as np
from datetime import datetime
from utils.logger import logger


class OCREngineONNX:
    """
    –î–≤–∏–∂–æ–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ ONNX Runtime

    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    - –ü–æ—Å—Ç—Ä–æ—á–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    - –ü–∞—Ä—Å–∏–Ω–≥ —É—Ä–æ–≤–Ω–µ–π –∑–¥–∞–Ω–∏–π (Lv.X)
    - –ü–∞—Ä—Å–∏–Ω–≥ —Ç–∞–π–º–µ—Ä–æ–≤ (HH:MM:SS)
    - –ü–∞—Ä—Å–∏–Ω–≥ –Ω–∞–∑–≤–∞–Ω–∏–π –∑–¥–∞–Ω–∏–π
    - Debug —Ä–µ–∂–∏–º (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å bbox)

    Singleton: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ get_ocr_engine_onnx() –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞
    """

    def __init__(self, model_dir=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ONNX Runtime OCR

        Args:
            model_dir: –ø–∞–ø–∫–∞ —Å .onnx –º–æ–¥–µ–ª—è–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: data/models/onnx –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞)
        """
        try:
            import onnxruntime as ort
            from pathlib import Path

            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞
            if model_dir is None:
                # –¢–µ–∫—É—â–∏–π —Ñ–∞–π–ª: utils/ocr_engine_onnx.py
                # –ö–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞: –Ω–∞ 1 —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ
                current_file = Path(__file__)
                project_root = current_file.parent.parent
                model_dir = project_root / "data" / "models" / "onnx"

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ —Å—Ç—Ä–æ–∫—É
            self.model_dir = str(model_dir)
            self.debug_mode = False

            logger.debug(f"–ü—É—Ç—å –∫ –º–æ–¥–µ–ª—è–º ONNX: {self.model_dir}")

            # 1. –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
            self.providers = self._detect_providers(ort)

            # 2. –ó–∞–≥—Ä—É–∑–∏—Ç—å 3 –º–æ–¥–µ–ª–∏
            self.det_session = self._load_model(ort, 'det_model.onnx')
            self.rec_session = self._load_model(ort, 'rec_model.onnx')
            self.cls_session = self._load_model(ort, 'cls_model.onnx')

            # 3. –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å —Å–∏–º–≤–æ–ª–æ–≤
            self.char_dict = self._load_char_dict()

            logger.success(f"OCREngineONNX –≥–æ—Ç–æ–≤ (–ø—Ä–æ–≤–∞–π–¥–µ—Ä: {self.providers[0]})")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ONNX Runtime OCR: {e}")
            raise

    def _detect_providers(self, ort):
        """
        –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª—É—á—à–µ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:
        1. CUDAExecutionProvider (NVIDIA GPU)
        2. DmlExecutionProvider (AMD/Intel GPU)
        3. CPUExecutionProvider (Fallback)

        Args:
            ort: –º–æ–¥—É–ª—å onnxruntime

        Returns:
            list: ['CUDAExecutionProvider', 'CPUExecutionProvider']
        """
        available = ort.get_available_providers()
        logger.debug(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã ONNX: {available}")

        # –ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        priority = [
            'CUDAExecutionProvider',  # NVIDIA
            'DmlExecutionProvider',  # DirectML (AMD/Intel)
            'CPUExecutionProvider'  # CPU (–≤—Å–µ–≥–¥–∞ –µ—Å—Ç—å)
        ]

        # –í—ã–±—Ä–∞—Ç—å –ª—É—á—à–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π
        providers = [p for p in priority if p in available]

        if 'CUDAExecutionProvider' in providers:
            logger.success("‚úì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è NVIDIA GPU (CUDA)")
        elif 'DmlExecutionProvider' in providers:
            logger.success("‚úì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è DirectML GPU (AMD/Intel)")
        else:
            logger.info("‚úì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

        return providers

    def _load_model(self, ort, model_name):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç ONNX –º–æ–¥–µ–ª—å

        Args:
            ort: –º–æ–¥—É–ª—å onnxruntime
            model_name: –∏–º—è —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏

        Returns:
            InferenceSession: —Å–µ—Å—Å–∏—è ONNX
        """
        model_path = os.path.join(self.model_dir, model_name)

        if not os.path.exists(model_path):
            raise FileNotFoundError(
                f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}\n"
                f"–ó–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/download_onnx_models.py"
            )

        # –û–ø—Ü–∏–∏ —Å–µ—Å—Å–∏–∏ (–¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

        # –°–æ–∑–¥–∞—Ç—å —Å–µ—Å—Å–∏—é
        session = ort.InferenceSession(
            model_path,
            sess_options=sess_options,
            providers=self.providers
        )

        logger.debug(f"ONNX –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}")
        return session

    def _load_char_dict(self):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è recognition –º–æ–¥–µ–ª–∏

        Returns:
            dict: {–∏–Ω–¥–µ–∫—Å: —Å–∏–º–≤–æ–ª}
        """
        dict_path = os.path.join(self.model_dir, 'ppocr_keys_v1.txt')

        if not os.path.exists(dict_path):
            logger.warning(f"–°–ª–æ–≤–∞—Ä—å –Ω–µ –Ω–∞–π–¥–µ–Ω: {dict_path}")
            logger.warning("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å (—Ü–∏—Ñ—Ä—ã + –ª–∞—Ç–∏–Ω–∏—Ü–∞ + –∫–∏—Ä–∏–ª–ª–∏—Ü–∞)")

            # –ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å
            chars = []
            chars.extend([str(i) for i in range(10)])  # 0-9
            chars.extend([chr(i) for i in range(ord('a'), ord('z') + 1)])  # a-z
            chars.extend([chr(i) for i in range(ord('A'), ord('Z') + 1)])  # A-Z
            chars.extend([chr(i) for i in range(ord('–∞'), ord('—è') + 1)])  # –∞-—è
            chars.extend([chr(i) for i in range(ord('–ê'), ord('–Ø') + 1)])  # –ê-–Ø
            chars.extend(['—ë', '–Å'])
            chars.extend([' ', '.', ',', ':', '-', '(', ')', '/', '\\'])
        else:
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ —Ñ–∞–π–ª–∞
            with open(dict_path, 'r', encoding='utf-8') as f:
                chars = [line.strip() for line in f.readlines()]

        # –°–æ–∑–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å: –∏–Ω–¥–µ–∫—Å ‚Üí —Å–∏–º–≤–æ–ª
        char_dict = {i: char for i, char in enumerate(chars)}
        char_dict[len(chars)] = ' '  # –ü—Ä–æ–±–µ–ª –¥–ª—è blank –≤ CTC

        logger.debug(f"–°–ª–æ–≤–∞—Ä—å —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω: {len(chars)} —Å–∏–º–≤–æ–ª–æ–≤")
        return char_dict

    def set_debug_mode(self, enabled):
        """
        –í–∫–ª—é—á–∞–µ—Ç/–≤—ã–∫–ª—é—á–∞–µ—Ç debug —Ä–µ–∂–∏–º

        Args:
            enabled: True –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è, False –¥–ª—è –≤—ã–∫–ª—é—á–µ–Ω–∏—è
        """
        self.debug_mode = enabled
        logger.info(f"OCR debug —Ä–µ–∂–∏–º: {'–í–ö–õ' if enabled else '–í–´–ö–õ'}")

    def recognize_text(self, image, region=None, min_confidence=0.5):
        """
        –†–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏

        –ê–ª–≥–æ—Ä–∏—Ç–º:
        1. –î–µ—Ç–µ–∫—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ (det_model)
        2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ (cls_model)
        3. –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (rec_model)

        Args:
            image: numpy.ndarray (BGR —Ñ–æ—Ä–º–∞—Ç OpenCV)
            region: tuple (x1, y1, x2, y2) - –æ–±–ª–∞—Å—Ç—å –¥–ª—è OCR (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            min_confidence: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (0.0-1.0)

        Returns:
            list: –°–ø–∏—Å–æ–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            [
                {
                    'text': '–§–µ—Ä–º–∞ –ì—Ä—É–Ω—Ç–∞',
                    'confidence': 0.98,
                    'bbox': [[x1,y1], [x2,y2], [x3,y3], [x4,y4]],
                    'x': 120,  # –¶–µ–Ω—Ç—Ä –ø–æ X
                    'y': 582   # –¶–µ–Ω—Ç—Ä –ø–æ Y
                },
                ...
            ]
        """
        try:
            # –û–±—Ä–µ–∑–∞—Ç—å —Ä–µ–≥–∏–æ–Ω –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if region:
                x1, y1, x2, y2 = region
                image = image[y1:y2, x1:x2].copy()

            # 1. –î–µ—Ç–µ–∫—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤
            boxes = self._detect_text_regions(image)

            if not boxes or len(boxes) == 0:
                logger.debug("OCR –Ω–µ –æ–±–Ω–∞—Ä—É–∂–∏–ª —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ä–µ–≥–∏–æ–Ω—ã")
                return []

            logger.debug(f"OCR –æ–±–Ω–∞—Ä—É–∂–∏–ª {len(boxes)} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤")

            # 2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞
            elements = []
            for box in boxes:
                # –í—ã—Ä–µ–∑–∞—Ç—å —Ä–µ–≥–∏–æ–Ω
                region_img = self._crop_region(image, box)

                if region_img is None or region_img.size == 0:
                    continue

                # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ (–Ω—É–∂–Ω–æ –ª–∏ –ø–æ–≤–æ—Ä–∞—á–∏–≤–∞—Ç—å)
                angle = self._classify_orientation(region_img)
                if angle == 180:
                    region_img = cv2.rotate(region_img, cv2.ROTATE_180)

                # –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
                text, conf = self._recognize_region(region_img)

                # –§–∏–ª—å—Ç—Ä –ø–æ confidence
                if conf < min_confidence:
                    continue

                # –í—ã—á–∏—Å–ª–∏—Ç—å —Ü–µ–Ω—Ç—Ä bbox
                x_center = int((box[0][0] + box[2][0]) / 2)
                y_center = int((box[0][1] + box[2][1]) / 2)

                # –î–æ–±–∞–≤–∏—Ç—å —Å–º–µ—â–µ–Ω–∏–µ –µ—Å–ª–∏ –±—ã–ª region
                if region:
                    x_center += region[0]
                    y_center += region[1]
                    box = [[p[0] + region[0], p[1] + region[1]] for p in box]

                elements.append({
                    'text': text.strip(),
                    'confidence': float(conf),
                    'bbox': box,
                    'x': x_center,
                    'y': y_center
                })

            logger.debug(f"OCR —Ä–∞—Å–ø–æ–∑–Ω–∞–ª {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–ø–æ—Ä–æ–≥: {min_confidence})")
            return elements

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ OCR —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {e}")
            logger.exception(e)
            return []

    def _detect_text_regions(self, image):
        """
        –î–µ—Ç–µ–∫—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ (det_model)

        Args:
            image: numpy.ndarray (BGR)

        Returns:
            list: –°–ø–∏—Å–æ–∫ bbox [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
        """
        # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏
        img_resized, ratio_h, ratio_w = self._preprocess_det(image)

        # Inference
        input_name = self.det_session.get_inputs()[0].name
        output_name = self.det_session.get_outputs()[0].name

        outputs = self.det_session.run(
            [output_name],
            {input_name: img_resized}
        )

        # –ü–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
        boxes = self._postprocess_det(outputs[0], ratio_h, ratio_w, image.shape)

        return boxes

    def _preprocess_det(self, image):
        """
        –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–ª—è detection –º–æ–¥–µ–ª–∏

        Args:
            image: numpy.ndarray (BGR)

        Returns:
            tuple: (preprocessed_image, ratio_h, ratio_w)
        """
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å BGR ‚Üí RGB
        img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä (–º–∞–∫—Å —Å—Ç–æ—Ä–æ–Ω–∞ = 960, –∫—Ä–∞—Ç–Ω–æ 32)
        h, w = img.shape[:2]
        max_side = 960

        if max(h, w) > max_side:
            if h > w:
                new_h = max_side
                new_w = int(w * (max_side / h))
            else:
                new_w = max_side
                new_h = int(h * (max_side / w))
        else:
            new_h, new_w = h, w

        # –û–∫—Ä—É–≥–ª–∏—Ç—å –¥–æ –∫—Ä–∞—Ç–Ω–æ—Å—Ç–∏ 32
        new_h = (new_h // 32) * 32
        new_w = (new_w // 32) * 32

        # Resize
        img_resized = cv2.resize(img, (new_w, new_h))

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        img_normalized = img_resized.astype(np.float32) / 255.0
        mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)
        std = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)
        img_normalized = (img_normalized - mean) / std

        # Transpose (H, W, C) ‚Üí (C, H, W)
        img_transposed = img_normalized.transpose(2, 0, 1)

        # –î–æ–±–∞–≤–∏—Ç—å batch dimension
        img_batch = np.expand_dims(img_transposed, axis=0).astype(np.float32)

        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
        ratio_h = new_h / h
        ratio_w = new_w / w

        return img_batch, ratio_h, ratio_w

    def _postprocess_det(self, pred, ratio_h, ratio_w, original_shape):
        """
        –ü–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–ª—è detection –º–æ–¥–µ–ª–∏ (DBNet)

        Args:
            pred: –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏ (batch, 1, H, W)
            ratio_h: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤—ã—Å–æ—Ç—ã
            ratio_w: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —à–∏—Ä–∏–Ω—ã
            original_shape: –∏—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

        Returns:
            list: –°–ø–∏—Å–æ–∫ bbox [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
        """
        # –ò–∑–≤–ª–µ—á—å probability map
        pred_map = pred[0, 0, :, :]  # (H, W)

        # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è (–ø–æ—Ä–æ–≥ 0.3)
        mask = pred_map > 0.3
        mask = mask.astype(np.uint8) * 255

        # –ù–∞–π—Ç–∏ –∫–æ–Ω—Ç—É—Ä—ã
        contours, _ = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)

        boxes = []
        for contour in contours:
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—â–∞–¥—å
            if cv2.contourArea(contour) < 10:
                continue

            # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫
            rect = cv2.minAreaRect(contour)
            box = cv2.boxPoints(rect)

            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
            box[:, 0] = box[:, 0] / ratio_w
            box[:, 1] = box[:, 1] / ratio_h

            # Clip –∫ –≥—Ä–∞–Ω–∏—Ü–∞–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            box[:, 0] = np.clip(box[:, 0], 0, original_shape[1])
            box[:, 1] = np.clip(box[:, 1], 0, original_shape[0])

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ —Å–ø–∏—Å–æ–∫
            box = box.astype(np.int32).tolist()
            boxes.append(box)

        return boxes

    def _classify_orientation(self, image):
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ (cls_model)

        Args:
            image: numpy.ndarray (BGR)

        Returns:
            int: —É–≥–æ–ª (0 –∏–ª–∏ 180)
        """
        # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        img_preprocessed = self._preprocess_cls(image)

        # Inference
        input_name = self.cls_session.get_inputs()[0].name
        output_name = self.cls_session.get_outputs()[0].name

        outputs = self.cls_session.run(
            [output_name],
            {input_name: img_preprocessed}
        )

        # –ü–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
        prob = outputs[0][0]  # [prob_0, prob_180]
        angle = 0 if prob[0] > prob[1] else 180

        return angle

    def _preprocess_cls(self, image):
        """
        –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–ª—è classification –º–æ–¥–µ–ª–∏

        Args:
            image: numpy.ndarray (BGR)

        Returns:
            numpy.ndarray: preprocessed image (1, 3, 48, 192)
        """
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å BGR ‚Üí RGB
        img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Resize –¥–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ 192x48
        img_resized = cv2.resize(img, (192, 48))

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        img_normalized = img_resized.astype(np.float32) / 255.0
        mean = np.array([0.5, 0.5, 0.5]).reshape(1, 1, 3)
        std = np.array([0.5, 0.5, 0.5]).reshape(1, 1, 3)
        img_normalized = (img_normalized - mean) / std

        # Transpose (H, W, C) ‚Üí (C, H, W)
        img_transposed = img_normalized.transpose(2, 0, 1)

        # –î–æ–±–∞–≤–∏—Ç—å batch dimension
        img_batch = np.expand_dims(img_transposed, axis=0).astype(np.float32)

        return img_batch

    def _recognize_region(self, image):
        """
        –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Ä–µ–≥–∏–æ–Ω–µ (rec_model)

        Args:
            image: numpy.ndarray (BGR)

        Returns:
            tuple: (text, confidence)
        """
        # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–ª—è recognition
        img_preprocessed = self._preprocess_rec(image)

        # Inference
        input_name = self.rec_session.get_inputs()[0].name
        output_name = self.rec_session.get_outputs()[0].name

        outputs = self.rec_session.run(
            [output_name],
            {input_name: img_preprocessed}
        )

        # –ü–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ (CTC decode)
        text, conf = self._postprocess_rec(outputs[0])

        return text, conf

    def _preprocess_rec(self, image):
        """
        –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–ª—è recognition –º–æ–¥–µ–ª–∏

        Args:
            image: numpy.ndarray (BGR)

        Returns:
            numpy.ndarray: preprocessed image (1, 3, 48, W)
        """
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å BGR ‚Üí RGB
        img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã—Å–æ—Ç–∞ 48, —à–∏—Ä–∏–Ω–∞ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–∞
        h, w = img.shape[:2]
        target_h = 48
        target_w = int(w * (target_h / h))

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ —à–∏—Ä–∏–Ω–µ (–º–∞–∫—Å 320)
        if target_w > 320:
            target_w = 320

        # Resize
        img_resized = cv2.resize(img, (target_w, target_h))

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        img_normalized = img_resized.astype(np.float32) / 255.0
        mean = np.array([0.5, 0.5, 0.5]).reshape(1, 1, 3)
        std = np.array([0.5, 0.5, 0.5]).reshape(1, 1, 3)
        img_normalized = (img_normalized - mean) / std

        # Transpose (H, W, C) ‚Üí (C, H, W)
        img_transposed = img_normalized.transpose(2, 0, 1)

        # –î–æ–±–∞–≤–∏—Ç—å batch dimension
        img_batch = np.expand_dims(img_transposed, axis=0).astype(np.float32)

        return img_batch

    def _postprocess_rec(self, pred):
        """
        –ü–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–ª—è recognition –º–æ–¥–µ–ª–∏ (CTC decode)

        Args:
            pred: –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏ (batch, time_steps, num_classes)

        Returns:
            tuple: (text, confidence)
        """
        # CTC greedy decode
        pred_indices = pred[0].argmax(axis=1)  # (time_steps,)

        # –£–¥–∞–ª–∏—Ç—å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã –∏ blank
        chars = []
        confidences = []
        prev_idx = -1

        for idx in pred_indices:
            if idx != prev_idx and idx < len(self.char_dict):
                char = self.char_dict.get(idx, '')
                if char and char != ' ':  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º blank
                    chars.append(char)
                    confidences.append(pred[0, :, idx].max())
            prev_idx = idx

        text = ''.join(chars)
        conf = np.mean(confidences) if confidences else 0.0

        return text, conf

    def _crop_region(self, image, box):
        """
        –í—ã—Ä–µ–∑–∞–µ—Ç —Ä–µ–≥–∏–æ–Ω –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ bbox

        Args:
            image: numpy.ndarray (BGR)
            box: bbox [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]

        Returns:
            numpy.ndarray: –≤—ã—Ä–µ–∑–∞–Ω–Ω—ã–π —Ä–µ–≥–∏–æ–Ω
        """
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ numpy array
        box = np.array(box, dtype=np.float32)

        # –ü–æ–ª—É—á–∏—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫
        rect = cv2.minAreaRect(box)
        box_points = cv2.boxPoints(rect).astype(np.int32)

        # –í—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä—ã
        width = int(rect[1][0])
        height = int(rect[1][1])

        if width <= 0 or height <= 0:
            return None

        # –¶–µ–ª–µ–≤—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (–ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫)
        dst_points = np.array([
            [0, 0],
            [width - 1, 0],
            [width - 1, height - 1],
            [0, height - 1]
        ], dtype=np.float32)

        # –ú–∞—Ç—Ä–∏—Ü–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        M = cv2.getPerspectiveTransform(box, dst_points)

        # –í—ã—Ä–æ–≤–Ω—è—Ç—å —Ä–µ–≥–∏–æ–Ω
        warped = cv2.warpPerspective(image, M, (width, height))

        return warped

    # ===== –ú–ï–¢–û–î–´ –ü–ê–†–°–ò–ù–ì–ê (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô –ò–ó –¢–ï–ö–£–©–ï–ô –í–ï–†–°–ò–ò) =====

    def group_by_rows(self, elements, y_threshold=20):
        """
        –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å—Ç—Ä–æ–∫–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ Y-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç

        Args:
            elements: —Å–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–∑ recognize_text()
            y_threshold: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –ø–æ Y –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ (–ø–∏–∫—Å–µ–ª–µ–π)

        Returns:
            list: –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫
        """
        if not elements:
            return []

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ Y (—Å–≤–µ—Ä—Ö—É –≤–Ω–∏–∑)
        sorted_elements = sorted(elements, key=lambda e: e['y'])

        rows = []
        current_row = [sorted_elements[0]]
        current_y = sorted_elements[0]['y']

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
        for element in sorted_elements[1:]:
            if abs(element['y'] - current_y) <= y_threshold:
                current_row.append(element)
            else:
                current_row.sort(key=lambda e: e['x'])
                rows.append(current_row)
                current_row = [element]
                current_y = element['y']

        # –î–æ–±–∞–≤–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É
        if current_row:
            current_row.sort(key=lambda e: e['x'])
            rows.append(current_row)

        logger.debug(f"–≠–ª–µ–º–µ–Ω—Ç—ã —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –≤ {len(rows)} —Å—Ç—Ä–æ–∫ (threshold: {y_threshold}px)")
        return rows

    def parse_level(self, text):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –∏–∑ —Ç–µ–∫—Å—Ç–∞ 'Lv.X'

        Args:
            text: —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞

        Returns:
            int –∏–ª–∏ None
        """
        pattern = r'L\s*v\s*\.\s*(\d+)'
        match = re.search(pattern, text, re.IGNORECASE)

        if match:
            level = int(match.group(1))
            logger.debug(f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω —É—Ä–æ–≤–µ–Ω—å: {level} –∏–∑ '{text}'")
            return level

        return None

    def parse_timer(self, text):
        """
        –ü–∞—Ä—Å–∏—Ç —Ç–∞–π–º–µ—Ä HH:MM:SS –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ —Å–µ–∫—É–Ω–¥—ã

        Args:
            text: —Å—Ç—Ä–æ–∫–∞ —Å —Ç–∞–π–º–µ—Ä–æ–º

        Returns:
            int (—Å–µ–∫—É–Ω–¥—ã) –∏–ª–∏ None
        """
        pattern = r'(\d{1,2}):(\d{2}):(\d{2})'
        match = re.search(pattern, text)

        if match:
            hours = int(match.group(1))
            minutes = int(match.group(2))
            seconds = int(match.group(3))

            if minutes > 59 or seconds > 59:
                logger.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–∞–π–º–µ—Ä: {text}")
                return None

            total_seconds = hours * 3600 + minutes * 60 + seconds
            logger.debug(f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω —Ç–∞–π–º–µ—Ä: {text} = {total_seconds} —Å–µ–∫—É–Ω–¥")
            return total_seconds

        return None

    def parse_building_name(self, text):
        """
        –û—á–∏—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∑–¥–∞–Ω–∏—è –æ—Ç 'Lv.X', '–ü–µ—Ä–µ–π—Ç–∏' –∏ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞

        Args:
            text: —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏

        Returns:
            str: –æ—á–∏—â–µ–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        """
        text = re.sub(r'\s+L\s*v\s*\.\s*\d+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'\s*–ü–µ—Ä–µ–π—Ç–∏\s*', '', text, flags=re.IGNORECASE)
        text = text.replace('\n', ' ')
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()

        return text

    def parse_navigation_panel(self, screenshot, emulator_id=None):
        """
        –ü–∞—Ä—Å–∏—Ç –ø–∞–Ω–µ–ª—å –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –Ω–∞–∑–≤–∞–Ω–∏–µ ‚Üî —É—Ä–æ–≤–µ–Ω—å ‚Üî –∏–Ω–¥–µ–∫—Å

        Args:
            screenshot: numpy.ndarray (BGR —Ñ–æ—Ä–º–∞—Ç)
            emulator_id: int (–¥–ª—è debug, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
            list: –°–ø–∏—Å–æ–∫ –∑–¥–∞–Ω–∏–π
        """
        logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –ø–∞–Ω–µ–ª–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏...")

        # 1. OCR —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
        elements = self.recognize_text(screenshot, min_confidence=0.5)

        # Debug —Ä–µ–∂–∏–º
        if self.debug_mode and emulator_id is not None:
            self._save_debug_image(screenshot, elements, emulator_id, "navigation")

        if not elements:
            logger.warning("OCR –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞ –ø–∞–Ω–µ–ª–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏")
            return []

        # 2. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å—Ç—Ä–æ–∫–∞–º
        rows = self.group_by_rows(elements, y_threshold=20)

        # 3. –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
        buildings = []
        building_counters = {}

        for row in rows:
            full_text = ' '.join([elem['text'] for elem in row])

            level = self.parse_level(full_text)
            building_name = self.parse_building_name(full_text)

            if not building_name or level is None:
                continue

            if len(building_name) < 3:
                continue

            y_coord = int(sum([elem['y'] for elem in row]) / len(row))

            if building_name not in building_counters:
                building_counters[building_name] = 0
            building_counters[building_name] += 1

            index = building_counters[building_name]
            button_coord = (330, y_coord)

            buildings.append({
                'name': building_name,
                'level': level,
                'index': index,
                'y_coord': y_coord,
                'button_coord': button_coord
            })

        logger.success(f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ {len(buildings)} –∑–¥–∞–Ω–∏–π –≤ –ø–∞–Ω–µ–ª–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏")
        return buildings

    def _save_debug_image(self, image, elements, emulator_id, operation):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç debug —Å–∫—Ä–∏–Ω—à–æ—Ç —Å bbox –∏ —Ç–µ–∫—Å—Ç–æ–º

        Args:
            image: numpy.ndarray (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)
            elements: —Å–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–∑ recognize_text()
            emulator_id: int (ID —ç–º—É–ª—è—Ç–æ—Ä–∞)
            operation: str (—Ç–∏–ø –æ–ø–µ—Ä–∞—Ü–∏–∏)
        """
        try:
            debug_folder = "data/screenshots/debug/ocr"
            os.makedirs(debug_folder, exist_ok=True)

            debug_img = image.copy()

            if not elements:
                cv2.putText(debug_img, "NO TEXT DETECTED BY OCR", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                logger.warning("OCR debug: —Ç–µ–∫—Å—Ç –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏")
            else:
                for elem in elements:
                    bbox = elem['bbox']
                    text = elem['text']
                    confidence = elem['confidence']

                    if confidence >= 0.9:
                        color = (0, 255, 0)
                    elif confidence >= 0.7:
                        color = (0, 255, 255)
                    else:
                        color = (0, 0, 255)

                    points = np.array(bbox, dtype=np.int32)
                    cv2.polylines(debug_img, [points], True, color, 2)

                    x = int(bbox[0][0])
                    y = int(bbox[0][1]) - 5
                    label = f"{text} ({confidence:.2f})"

                    text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                    cv2.rectangle(debug_img,
                                  (x, y - text_size[1] - 5),
                                  (x + text_size[0], y),
                                  (0, 0, 0),
                                  -1)

                    cv2.putText(debug_img, label, (x, y - 2),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"emu{emulator_id}_{operation}_{timestamp}.png"
            filepath = os.path.join(debug_folder, filename)

            cv2.imwrite(filepath, debug_img)

            abs_path = os.path.abspath(filepath)
            logger.info(f"OCR debug —Å–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {abs_path}")
            print(f"   üìÅ {abs_path}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è OCR debug —Å–∫—Ä–∏–Ω—à–æ—Ç–∞: {e}")


# ============================================
# Singleton
# ============================================

_ocr_onnx_instance = None


def get_ocr_engine_onnx():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä OCREngineONNX (Singleton)

    Returns:
        OCREngineONNX: –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
    """
    global _ocr_onnx_instance

    if _ocr_onnx_instance is None:
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ OCREngineONNX (Singleton)...")
        _ocr_onnx_instance = OCREngineONNX()
        logger.success("OCREngineONNX –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")

    return _ocr_onnx_instance